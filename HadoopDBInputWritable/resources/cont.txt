In this article we are going to explain how to read the Database table using Hadoop MapReduce.
Usually in ETL world we have to read the Relational databases and we also have to process massive amount of data using 
Hadoop MapReduce.

Now cosider a hypothetical case where we are having a online retail store's database and we are having a  
User table which contains millions of rows and we are interested in reading that table using 
Mapreduce.

-----User table defination---

In this article we are using MySql as database, while you may use any relevant RDBMS.

In this article we are going to implement custom InputWritable which will enable us to read the data rows from Database table.
In order to implement Custom input writable, we are going to implement DBWritable interface.

so, our DBInputWritable will look like following code,

In order to make the Database table reading simpler, we are just reading the User table using Mapper class and we 
are just writing the details to Text files in HDFS.
So this example also serves the purpose of Reading the Data from RDBMS and Writing into HDFS using MapReduce.

In order to provide the related RDBMS' connector, we need to place the relevant jar file to Hadoop's lib folder,
so in our case we have copied the mysql-connector-java-5.1.16.jar file to HADOOP's lib folder.
Once you copied the jar file, you need to restart the cluster.







