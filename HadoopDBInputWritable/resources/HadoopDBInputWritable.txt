In this article we are going to explain how to access the Database using Hadoop MapReduce.
Nowadays, in ETL world reading the Database and processing millions of records is perhaps the principle operation.
So in our hadoop tutorial journey, we are going to explain simple way to read database table using Hadoop MapReduce.

Consider a hypothetical case where we are having an online retail store's database and we are having a  
User table which contains millions of rows and we are interested in reading that table using 
Mapreduce.

For sake of simplicity we are going to read the table and we will emmit the records in HDFS as key value pairs.
So this Hadoop MapReduce tutorial serves as a base for reading RDBMS using Hadoop MapReduce where our data source is MySQL database
and sink is HDFS.

-----User table definition---

In our User table we are having 3 columns named as, user_id, user_name, department

Hadoop does provides various data types like IntWritable,FloatWritable, DoubleWritable etc,
but in our case we are going to implement custom InputWritable which will enable us to read the data rows from Database table.
In order to implement Custom input writable, we have to implement DBWritable interface.

so, our DBInputWritable will look like following code,

In order to make the Database table reading simpler, we are just reading the User table using Mapper class and we 
are just writing the details to Text files in HDFS.

Reading RDBMS required database connection, so we need to place the relevant jar file to Hadoop's lib folder,
so in our case we have copied the mysql-connector-java-5.1.16.jar file to HADOOP's lib folder.
Once you copied the jar file, you need to restart the cluster.

so now we need to build the Jar file with dependencies and then we can run the job using Hadoop jar command.
